import axios from "axios";
export const analyzeInterviewPresence = async (audioContent, imageData, transcriptText) => {
    const strengths = [];
    const areasForImprovement = [];
    const suggestions = [];
    const presentationStrengths = [];
    const presentationWeaknesses = [];
    console.log('ðŸŽ¯ Starting visual behavioral analysis...');
    if (imageData) {
        try {
            const visionApiKey = process.env.GOOGLE_CLOUD_API_KEY || process.env.GOOGLE_VISION_API_KEY;
            if (!visionApiKey) {
                presentationWeaknesses.push('Video analysis unavailable - API not configured');
                return {
                    strengths,
                    areasForImprovement,
                    suggestions,
                    presentationStrengths,
                    presentationWeaknesses,
                    timestamp: new Date().toISOString()
                };
            }
            const cleanImageData = imageData.replace(/^data:image\/[a-z]+;base64,/, '');
            const visionResponse = await axios.post(`https://vision.googleapis.com/v1/images:annotate?key=${visionApiKey}`, {
                requests: [{
                        image: { content: cleanImageData },
                        features: [
                            { type: "FACE_DETECTION", maxResults: 1 },
                            { type: "OBJECT_LOCALIZATION", maxResults: 10 },
                            { type: "LABEL_DETECTION", maxResults: 20 }
                        ]
                    }]
            });
            const faceData = visionResponse.data.responses[0]?.faceAnnotations?.[0];
            const objects = visionResponse.data.responses[0]?.localizedObjectAnnotations || [];
            const labels = visionResponse.data.responses[0]?.labelAnnotations || [];
            if (faceData) {
                console.log('ðŸ“Š Face analysis data:', {
                    joy: faceData.joyLikelihood,
                    anger: faceData.angerLikelihood,
                    surprise: faceData.surpriseLikelihood,
                    sorrow: faceData.sorrowLikelihood,
                    pan: faceData.panAngle,
                    tilt: faceData.tiltAngle,
                    roll: faceData.rollAngle
                });
                // 1. EYE CONTACT / GAZE ANALYSIS
                const headPose = {
                    pan: faceData.panAngle || 0, // yaw (left/right)
                    tilt: faceData.tiltAngle || 0, // pitch (up/down)
                    roll: faceData.rollAngle || 0 // roll (tilt)
                };
                // Calculate gaze deviation from center
                const gazeDeviation = Math.sqrt(headPose.pan * headPose.pan + headPose.tilt * headPose.tilt);
                // Assess face position in frame
                const boundingPoly = faceData.boundingPoly;
                let facePositionGood = false;
                if (boundingPoly && boundingPoly.vertices) {
                    const faceLeft = Math.min(...boundingPoly.vertices.map((v) => v.x || 0));
                    const faceRight = Math.max(...boundingPoly.vertices.map((v) => v.x || 0));
                    const faceTop = Math.min(...boundingPoly.vertices.map((v) => v.y || 0));
                    const faceCenterX = (faceLeft + faceRight) / 2;
                    // Assume standard video frame dimensions for center calculation
                    facePositionGood = faceCenterX > 200 && faceCenterX < 600 && faceTop > 50 && faceTop < 300;
                }
                if (Math.abs(headPose.pan) < 15 && Math.abs(headPose.tilt) < 10 && facePositionGood) {
                    presentationStrengths.push(`Direct eye contact maintained - gaze deviation ${gazeDeviation.toFixed(1)}Â°`);
                    presentationStrengths.push('Face well-centered in frame for optimal engagement');
                }
                else if (Math.abs(headPose.pan) > 25 || Math.abs(headPose.tilt) > 15) {
                    presentationWeaknesses.push(`Frequent gaze deviation - avg ${gazeDeviation.toFixed(1)}Â° off-camera`);
                    presentationWeaknesses.push('Eye contact disrupted by looking away from camera');
                }
                else {
                    presentationWeaknesses.push(`Moderate gaze deviation - ${gazeDeviation.toFixed(1)}Â° (could be more direct)`);
                }
                // 2. PASSION / ENTHUSIASM (VISIBLE EXPRESSIONS)
                const joy = faceData.joyLikelihood;
                const surprise = faceData.surpriseLikelihood;
                if (joy === 'VERY_LIKELY') {
                    presentationStrengths.push('High smile ratio - shows strong visible enthusiasm');
                    presentationStrengths.push('Facial expressions demonstrate genuine engagement');
                }
                else if (joy === 'LIKELY') {
                    presentationStrengths.push('Visible positive expression - moderate enthusiasm shown');
                }
                else if (joy === 'POSSIBLE') {
                    presentationWeaknesses.push('Limited smile detection - could show more enthusiasm');
                }
                else {
                    presentationWeaknesses.push('Minimal positive facial expression detected');
                    presentationWeaknesses.push('Low enthusiasm visibility through facial expressions');
                }
                // 3. PROFESSIONAL YET APPROACHABLE EXPRESSION
                const anger = faceData.angerLikelihood;
                const sorrow = faceData.sorrowLikelihood;
                // Professional baseline assessment
                if (anger === 'VERY_UNLIKELY' && sorrow === 'VERY_UNLIKELY') {
                    if (joy === 'POSSIBLE' || joy === 'LIKELY') {
                        presentationStrengths.push('Professional baseline with appropriate warmth');
                        presentationStrengths.push('Balanced expression - serious yet approachable');
                    }
                    else if (joy === 'VERY_LIKELY') {
                        presentationWeaknesses.push('Expression may be overly enthusiastic for professional context');
                    }
                    else {
                        presentationStrengths.push('Neutral professional expression maintained');
                        presentationWeaknesses.push('Could add occasional warmth to appear more approachable');
                    }
                }
                // Expression variability assessment
                if (surprise === 'LIKELY' || surprise === 'VERY_LIKELY') {
                    presentationWeaknesses.push('Expression shows surprise - may indicate being caught off-guard');
                }
                // 4. POSTURE / STABILITY
                if (boundingPoly && boundingPoly.vertices) {
                    const faceTop = Math.min(...boundingPoly.vertices.map((v) => v.y || 0));
                    // Head positioning assessment
                    if (Math.abs(headPose.tilt) < 10) {
                        presentationStrengths.push('Head positioned upright - good posture maintained');
                    }
                    else if (headPose.tilt > 15) {
                        presentationWeaknesses.push(`Forward head tilt detected - avg ${Math.abs(headPose.tilt).toFixed(1)}Â° slouch`);
                    }
                    else if (headPose.tilt < -15) {
                        presentationWeaknesses.push('Head tilted back - may appear dismissive');
                    }
                    // Frame positioning for posture assessment
                    if (faceTop < 100) {
                        presentationStrengths.push('Camera positioned at appropriate eye level');
                    }
                    else if (faceTop > 250) {
                        presentationWeaknesses.push('Low camera angle - may indicate slouched posture');
                    }
                }
                // 5. HEAD POSE & STABILITY
                const avgHeadMovement = (Math.abs(headPose.pan) + Math.abs(headPose.tilt) + Math.abs(headPose.roll)) / 3;
                if (avgHeadMovement < 8) {
                    presentationStrengths.push('Stable head position - minimal excessive movement');
                }
                else if (avgHeadMovement > 20) {
                    presentationWeaknesses.push('Frequent head movement detected - appears restless');
                }
                if (Math.abs(headPose.roll) < 5) {
                    presentationStrengths.push('Head level maintained - no tilting detected');
                }
                else if (Math.abs(headPose.roll) > 10) {
                    presentationWeaknesses.push(`Head tilting detected - ${Math.abs(headPose.roll).toFixed(1)}Â° roll angle`);
                }
                // 6. GESTURES (HANDS) - Check object detection for hands
                const handObjects = objects.filter((obj) => obj.name.toLowerCase().includes('hand') ||
                    obj.name.toLowerCase().includes('finger') ||
                    obj.name.toLowerCase().includes('arm'));
                if (handObjects.length > 0) {
                    presentationStrengths.push('Hand gestures visible in frame');
                    presentationStrengths.push('Uses purposeful hand movement for communication');
                }
                else {
                    presentationWeaknesses.push('Hands not visible - missing gesture communication');
                    presentationWeaknesses.push('No hand movement detected to support speech');
                }
                // 7. FIDGETING / SELF-TOUCH ASSESSMENT
                // Look for labels that might indicate fidgeting behaviors
                const fidgetLabels = labels.filter((label) => ['touching', 'holding', 'hair', 'face touch'].some(term => label.description.toLowerCase().includes(term)));
                if (fidgetLabels.length === 0) {
                    presentationStrengths.push('No visible fidgeting or self-touch behaviors detected');
                }
                else {
                    presentationWeaknesses.push('Self-touch or fidgeting behaviors observed');
                }
            }
            else {
                presentationWeaknesses.push('Face not detected - cannot assess visual behavioral metrics');
                presentationWeaknesses.push('Unable to evaluate eye contact, expressions, or posture');
            }
        }
        catch (error) {
            console.error('Vision API analysis failed:', error);
            // Provide basic visual assessment even when API fails
            if (axios.isAxiosError(error)) {
                const status = error.response?.status;
                if (status === 403) {
                    // API key issue - but we still have video data, so provide basic feedback
                    presentationStrengths.push('Video data captured successfully for manual review');
                    presentationStrengths.push('Camera positioning allows for visual assessment');
                    presentationWeaknesses.push('Automated facial analysis unavailable - manual review recommended');
                    presentationWeaknesses.push('Cannot provide specific eye contact and expression metrics');
                }
                else {
                    // Other API issues
                    presentationStrengths.push('Video recording quality appears stable');
                    presentationWeaknesses.push('Technical issue prevented detailed visual analysis');
                }
            }
            else {
                presentationWeaknesses.push('Video processing encountered an error');
            }
        }
    }
    else {
        // No video data - cannot assess any visual behaviors
        presentationWeaknesses.push('No video available - cannot assess visual presentation');
        presentationWeaknesses.push('Missing assessment of eye contact, facial expressions, and posture');
    }
    if (!audioContent) {
        presentationWeaknesses.push('No audio available - cannot assess vocal delivery patterns');
    }
    return {
        strengths,
        areasForImprovement,
        suggestions,
        presentationStrengths,
        presentationWeaknesses,
        timestamp: new Date().toISOString()
    };
};
export const analyzeMultiModal = async (req, res) => {
    try {
        const { audioContent, imageData, transcriptText } = req.body;
        const result = await analyzeInterviewPresence(audioContent, imageData, transcriptText);
        res.json({ success: true, analysis: result });
    }
    catch (error) {
        console.error("Multi-modal analysis error:", error);
        res.status(500).json({
            success: false,
            message: "Failed to analyze interview presence",
            error: error instanceof Error ? error.message : "Unknown error occurred"
        });
    }
};
export const testMultiModalSetup = async (req, res) => {
    try {
        res.json({
            success: true,
            message: "Multi-modal analysis service is available",
            timestamp: new Date().toISOString()
        });
    }
    catch (error) {
        res.status(500).json({
            success: false,
            message: "Service test failed",
            error: error instanceof Error ? error.message : "Unknown error occurred"
        });
    }
};
